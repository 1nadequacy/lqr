{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from video import VideoRecorder\n",
    "import dmc2gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dmc2gym.make('point_mass', 'easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, obs_dim, action_dim, device, capacity):\n",
    "        self.device = device\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.obses = np.empty((capacity, obs_dim), dtype=np.float32)\n",
    "        self.next_obses = np.empty((capacity, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.empty((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
    "        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        np.copyto(self.obses[self.idx], obs)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_obses[self.idx], next_obs)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.full = self.full or self.idx == 0\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(\n",
    "            0, self.capacity if self.full else self.idx, size=batch_size)\n",
    "\n",
    "        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n",
    "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
    "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n",
    "        next_obses = torch.as_tensor(\n",
    "            self.next_obses[idxs], device=self.device).float()\n",
    "        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n",
    "\n",
    "        return obses, actions, rewards, next_obses, not_dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, control_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.F = nn.Parameter(torch.rand(state_dim, state_dim + control_dim))\n",
    "        self.f = nn.Parameter(torch.rand(state_dim))\n",
    "        \n",
    "    def forward(self, x, u):\n",
    "        # f(x, u) = F * [x u]^T + f\n",
    "        xu = torch.cat([x, u], dim=-1)\n",
    "        return xu.matmul(self.F.t()) + self.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostModel(nn.Module):\n",
    "    def __init__(self, state_dim, control_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C = nn.Parameter(torch.eye(state_dim + control_dim) * 0.001)\n",
    "        self.C.data[0, 0] = 1\n",
    "        self.C.data[1, 1] = 1\n",
    "        self.c = nn.Parameter(torch.zeros(state_dim + control_dim))\n",
    "        \n",
    "    def forward(self, x, u):\n",
    "        # c(x, u) = 0.5 * [x u] * C * [x u]^T + [x u] * c\n",
    "        xu = torch.cat([x, u], dim=-1)\n",
    "        return 0.5 * ((xu @ self.C.t()) * xu).sum(dim=1) + xu.matmul(self.c.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQR(object):\n",
    "    def __init__(self, state_dim, control_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.control_dim = control_dim\n",
    "        \n",
    "    def _backward_pass(self, F, f, C, c, T):\n",
    "        V = torch.zeros(self.state_dim, self.state_dim)\n",
    "        v = torch.zeros(self.state_dim)\n",
    "        Ks, ks = [], []\n",
    "        N = self.state_dim\n",
    "        \n",
    "        for t in range(T - 1, 0, -1):\n",
    "            Q = C + F.t() @ V @ F\n",
    "            q = c + F.t() @ V @ f + F.t() @ v\n",
    "\n",
    "            Qxx, Qxu, Qux, Quu = Q[:N, :N], Q[:N, N:], Q[N:, :N], Q[N:, N:]\n",
    "            qx, qu = q[:N], q[N:]\n",
    "\n",
    "            Quu_inv = torch.inverse(Quu) \n",
    "            K = - Quu_inv @ Qux\n",
    "            k = - Quu_inv @ qu\n",
    "\n",
    "            Ks.append(K)\n",
    "            ks.append(k)\n",
    "\n",
    "            V = Qxx + Qxu @ K + K.t() @ Qux + K.t() @ Quu @ K\n",
    "            v = qx + Qxu @ k + K.t() @ qu + K.t() @ Quu @ k\n",
    "        \n",
    "        return Ks, ks\n",
    "    \n",
    "    def _forward_pass(self, x, Ks, ks, trans_model):\n",
    "        us = []\n",
    "        for K, k in zip(Ks, ks):\n",
    "            u = K @ x + k\n",
    "            u = u.clamp(-1, 1)\n",
    "            us.append(u)\n",
    "            x = trans_model(x, u)\n",
    "        return us\n",
    "        \n",
    "        \n",
    "    def plan(self, x, T, trans_model, cost_model):\n",
    "        F, f = trans_model.F.detach(), trans_model.f.detach()\n",
    "        C, c = cost_model.C.detach(), cost_model.c.detach()\n",
    "        \n",
    "        Ks, ks = self._backward_pass(F, f, C, c, T)\n",
    "        us = self._forward_pass(x, Ks, ks, trans_model)\n",
    "        \n",
    "        return us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "control_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = TransitionModel(state_dim, control_dim)\n",
    "cost_model = CostModel(state_dim, control_dim)\n",
    "lqr = LQR(state_dim, control_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(state_dim, control_dim, device, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    x = env.reset()\n",
    "    u = env.action_space.sample()\n",
    "    next_x, reward, done, _ = env.step(u)\n",
    "    buffer.add(x, u, reward, next_x, float(done))\n",
    "    x = next_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_opt = torch.optim.Adam(trans_model.parameters())\n",
    "cost_opt = torch.optim.Adam(cost_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 step: 102 reward: 0.000 trans loss: 0.124 cost loss: 0.000\n",
      "episode: 1 step: 441 reward: 0.000 trans loss: 0.002 cost loss: 0.000\n",
      "episode: 2 step: 213 reward: 0.000 trans loss: 0.003 cost loss: 0.000\n",
      "episode: 3 step: 319 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 4 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 5 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 6 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 7 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 8 step: 998 reward: 0.269 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 9 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 10 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 11 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 12 step: 998 reward: 3.576 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 13 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 14 step: 998 reward: 0.221 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 15 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 16 step: 998 reward: 0.077 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 17 step: 998 reward: 23.179 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 18 step: 998 reward: 7.911 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 19 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 20 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 21 step: 998 reward: 0.007 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 22 step: 998 reward: 40.150 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 23 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 24 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 25 step: 998 reward: 0.064 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 26 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 27 step: 998 reward: 0.892 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 28 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 29 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 30 step: 998 reward: 0.036 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 31 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 32 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 33 step: 998 reward: 17.294 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 34 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 35 step: 998 reward: 15.871 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 36 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 37 step: 998 reward: 0.213 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 38 step: 998 reward: 5.350 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 39 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 40 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 41 step: 998 reward: 0.001 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 42 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 43 step: 998 reward: 97.098 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 44 step: 998 reward: 83.827 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 45 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 46 step: 998 reward: 16.299 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 47 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 48 step: 998 reward: 116.797 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 49 step: 998 reward: 10.220 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 50 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 51 step: 998 reward: 1.554 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 52 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 53 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 54 step: 998 reward: 0.161 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 55 step: 998 reward: 154.870 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 56 step: 998 reward: 2.519 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 57 step: 998 reward: 475.075 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 58 step: 998 reward: 0.127 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 59 step: 998 reward: 17.226 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 60 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 61 step: 998 reward: 45.639 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 62 step: 998 reward: 8.020 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 63 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 64 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 65 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 66 step: 998 reward: 143.664 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 67 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 68 step: 998 reward: 8.262 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 69 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 70 step: 998 reward: 9.465 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 71 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 72 step: 998 reward: 0.045 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 73 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 74 step: 998 reward: 368.904 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 75 step: 998 reward: 263.299 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 76 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 77 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 78 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 79 step: 998 reward: 40.669 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 80 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 81 step: 998 reward: 144.362 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 82 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 83 step: 998 reward: 5.135 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 84 step: 998 reward: 325.430 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 85 step: 998 reward: 0.010 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 86 step: 998 reward: 135.511 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 87 step: 998 reward: 13.351 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 88 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 89 step: 998 reward: 101.522 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 90 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 91 step: 998 reward: 0.090 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 92 step: 998 reward: 162.354 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 93 step: 998 reward: 0.241 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 94 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 95 step: 998 reward: 21.875 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 96 step: 998 reward: 24.719 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 97 step: 998 reward: 0.002 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 98 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 99 step: 998 reward: 30.458 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 100 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 101 step: 998 reward: 2.440 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 102 step: 998 reward: 35.338 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 103 step: 998 reward: 245.432 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 104 step: 998 reward: 0.991 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 105 step: 998 reward: 0.092 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 106 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 107 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 108 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 109 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 110 step: 998 reward: 233.751 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 111 step: 998 reward: 150.960 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 112 step: 998 reward: 1.181 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 113 step: 998 reward: 93.018 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 114 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 115 step: 998 reward: 51.662 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 116 step: 998 reward: 75.119 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 117 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 118 step: 998 reward: 0.573 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 119 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 120 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 121 step: 998 reward: 228.103 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 122 step: 998 reward: 0.008 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 123 step: 998 reward: 492.167 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 124 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 125 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 126 step: 998 reward: 74.081 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 127 step: 998 reward: 43.034 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 128 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 129 step: 998 reward: 369.864 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 130 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 131 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 132 step: 998 reward: 287.763 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 133 step: 998 reward: 416.547 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 134 step: 998 reward: 153.724 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 135 step: 998 reward: 133.075 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 136 step: 998 reward: 164.917 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 137 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 138 step: 998 reward: 0.000 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 139 step: 998 reward: 20.243 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 140 step: 998 reward: 767.515 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 141 step: 998 reward: 0.178 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 142 step: 998 reward: 0.228 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 143 step: 998 reward: 91.695 trans loss: 0.000 cost loss: 0.000\n",
      "episode: 144 step: 998 reward: 20.619 trans loss: 0.000 cost loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "for it in range(1000):\n",
    "    \n",
    "    total_trans_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    U = 1000\n",
    "    for _ in range(U):\n",
    "        x, u, r, nx, _ = buffer.sample(64)\n",
    "    \n",
    "        trans_opt.zero_grad()\n",
    "        trans_loss = F.mse_loss(trans_model(x, u), nx)\n",
    "        trans_loss.backward()\n",
    "        trans_opt.step()\n",
    "\n",
    "        #cost_opt.zero_grad()\n",
    "        #cost_loss = F.mse_loss(cost_model(x, u), 1. - r)\n",
    "        #cost_loss.backward()\n",
    "        #cost_opt.step()\n",
    "        \n",
    "        total_trans_loss += trans_loss.item()\n",
    "        #total_cost_loss += cost_loss.item()\n",
    "        \n",
    "    total_cost_loss /= U\n",
    "    total_trans_loss /= U\n",
    "        \n",
    "    \n",
    "    video = VideoRecorder(env, enabled=it % 10 == 0)\n",
    "    x = env.reset()\n",
    "    us = lqr.plan(torch.as_tensor(x).float(), 1000, trans_model, cost_model)\n",
    "    \n",
    "    total_reward = 0\n",
    "    for j, u in enumerate(us):\n",
    "        \n",
    "        u = u.detach().numpy().clip(-0.999, 0.999)\n",
    "        if np.isnan(u[0]):\n",
    "            break\n",
    "        next_x, reward, done, _ = env.step(u)\n",
    "        video.record()\n",
    "        total_reward += reward\n",
    "        \n",
    "        buffer.add(x, u, reward, next_x, float(done))\n",
    "        x = next_x\n",
    "        \n",
    "    video.save('video', '%d.mp4' % it)\n",
    "    \n",
    "    \n",
    "    print('episode: %d step: %d reward: %.3f trans loss: %.3f cost loss: %.3f' % (it, j, total_reward, total_trans_loss, total_cost_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7701,  0.0489,  0.1073,  0.1030, -0.0519,  0.1217],\n",
       "        [ 0.1691,  0.4826,  0.1812, -0.5827, -0.0950,  0.3328],\n",
       "        [-0.0354, -0.0146,  0.5991,  0.1599,  0.0305, -0.0157],\n",
       "        [ 0.1713, -0.1617,  0.7018, -0.3640,  0.2524, -0.2258]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 6], m2: [4 x 6] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c7fb2b8405c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 6], m2: [4 x 6] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "torch.mm(torch.cat([x, u], dim=-1), tran.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ad11d65b0eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xu' is not defined"
     ]
    }
   ],
   "source": [
    "xu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CostModel(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(x, u).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xu.matmul(c.c.t()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
